\documentclass{standalone}
\begin{document}
	\section{Accuracy Comparison}	
	
	In order to check the performances of the pipeline, I've performed a comparison between the areas estimated by the pipeline itself and the one belonging from the manual segmentation. In order to match the labels and found how much they are according, I've used the Intersection over Union metrics, also known as Jaccard score. The intersection over union is a very suitable metrics in this case, mostly because allows  to overcome the issue of the over-represented background:\\
	Since the number of voxel of the non-GGO is several order of magnitude higher than the GGO ones, metrics like Pixel Accuracy will fails.
	IoU encodes the shape properties of the objects under comparison, e.g. the widths, heights and locations of two bounding boxes, into the region property and then calculates a normalized measure that focuses on their areas (or volumes); in this way the IoU is invariant to the scale of the considered problem. 
	The Intersection over Union is defined as follows: 
	\begin{equation*}
		IoU = \frac{\|A \cap B \|}{\| A \cup B \|}
	\end{equation*}
	Where $A,\, B\in S\subseteq\mathbb{R}$ are, in our case, the estimated labels and the ground truth.\\

	An other test that was made was to segment scans from healty patients and check that no GGO was detected. This kind of test was made on the $5$ healthy patients from Stant'Orsola and on the $CT-0$ group of MOSMED dataset.\\
	
	\subsubsection*{Healthy Control}
	
	\subsubsection*{MOSMED}
	
	\subsubsection*{ZENODO}
	
	\subsubsection*{Sant'Orsola}

\end{document}